# Fixture Timing & Benchmarking

## Running Timing Analysis

### Comprehensive Timing Report (Recommended)
```bash
./scripts/timings.sh
```

Runs fixture tests with full timing instrumentation and generates a comprehensive report including:
- **Per-category timing breakdown** - Fixture timing by category (context, executor, tools, etc.)
- **Function-level timing** - Hierarchical tracing of instrumented async functions
- **Slowest individual fixtures** (>= 1.0s) - Identify performance bottlenecks
- **Overall performance metrics** - Sequential time, wall clock, parallelization efficiency
- **Optional flamegraph generation** - Visual CPU profiling (requires Linux/WSL)

**Options:**
```bash
./scripts/timings.sh                             # Full timing report
./scripts/timings.sh --export timing_data.json   # Export timing data to JSON
./scripts/timings.sh --flamegraph                # Generate flamegraph profile
```

### Per-Category Breakdown (Manual)
```bash
cargo test -p integration-tests test_all_fixtures -- --nocapture 2>&1 | grep -A 30 "Per-Category"
```

Shows timing breakdown by fixture category (context, executor, tools, etc.).

### Tracing-Based Timing (Manual)
```bash
cargo test -p integration-tests test_all_fixtures -- --nocapture 2>&1 | grep -A 50 "Timing Report"
```

Shows hierarchical span timing for instrumented async functions.

### Flamegraph Profiling
```bash
./scripts/profile_flamegraph.sh
```

**Requirements**:
- Static linking (now enabled via `merlin-deps` rlib build)
- **Linux/WSL**: Uses `perf` for sampling
- **Windows**: Requires admin privileges or `dtrace`

**Alternative for Windows**: Use `samply` profiler instead:
```bash
cargo install samply
samply record cargo test -p integration-tests test_all_fixtures --release
```

## Current Performance (140 fixtures)

**Latest Results (2025-11-06):**
- **Wall clock**: ~11.8s (16 parallel workers)
- **Sequential time**: ~171s total across all fixtures
- **Parallelization efficiency**: ~14.5x speedup
- **Key optimization**: Conditional embedding initialization active

**After Conditional Embedding Optimization (2025-11-04):**
- **Wall clock**: ~10.5s (16 parallel workers)
- **Sequential time**: ~194s total across all fixtures
- **Parallelization efficiency**: ~18x speedup
- **Key optimization**: Only initialize embeddings for fixtures using pre-made test workspaces

**After TypeScript Caching (2025-01-04):**
- **Wall clock**: ~18s (16 parallel workers)
- **Sequential time**: ~275s total across all fixtures
- **Parallelization efficiency**: ~15x speedup

**Before Optimization (baseline):**
- **Wall clock**: ~17-18s (16 parallel workers)
- **Sequential time**: ~249s total across all fixtures
- **Parallelization efficiency**: 13-14x speedup

## Infrastructure

### 1. Hierarchical Span-Based Tracing

**Location**: `crates/integration-tests/src/timing.rs`

**What it does**:
- Collects hierarchical timing data for async function calls
- Uses `tracing-futures::Instrument` to track spans across `.await` points
- Exports timing tree and JSON for analysis

**Instrumented functions**:
- `execute_task()` - Top-level task execution
- `build_context_and_log()` - Context building
- `execute_with_step_executor()` - Step executor
- `execute_with_agent()` - Agent execution
- `execute_typescript_code()` - TypeScript execution
- `parse_agent_response_from_handle()` - Response parsing

**Components**:
- `TimingLayer` - Custom tracing subscriber layer
- `TimingData` - Hierarchical timing data structure
- `SpanTiming` - Individual span timing information

**Usage**:
```rust
let (timing_layer, timing_data) = TimingLayer::new();
let subscriber = tracing_subscriber::registry().with(timing_layer);
tracing::subscriber::set_global_default(subscriber).expect("Failed to set subscriber");

// ... run tests ...

let timing = timing_data.lock().expect("Lock poisoned");
timing.print_report(); // Hierarchical tree
timing.to_json(); // Export as JSON
```

### 2. Per-Category Timing Breakdown

**Location**: `crates/integration-tests/tests/fixture_tests.rs`

**What it does**:
- Automatically categorizes fixtures by directory (context/, executor/, tools/, etc.)
- Tracks execution time per fixture
- Aggregates statistics per category (count, total time, average)
- Sorts categories by total time to identify bottlenecks

**Output format**:
```
Category              Count      Total        Avg
----------------------------------------------------
context                  29     59.74s     2.060s
executor                 12     29.72s     2.477s
tools                    16     29.67s     1.855s
...
```

**Implementation**:
```rust
struct FixtureRunResult {
    result: Result<VerificationResult, String>,
    duration: std::time::Duration,
    category: String,
    name: String,
}

// Category extracted from parent directory
let category = fixture_path
    .parent()
    .and_then(|p| p.file_name())
    .and_then(|n| n.to_str())
    .map_or_else(|| "unknown".to_owned(), ToString::to_string);
```

### 3. Flamegraph Profiling

**Location**: `scripts/profile_flamegraph.sh`

**Build Configuration**:
- `merlin-deps` now builds both `dylib` and `rlib` crate types
- Dev builds prefer dylib (faster compilation)
- Release builds use rlib (static linking for flamegraph/LTO)

**Platform Support**:
- **Linux/WSL**: Full support via `perf`
- **Windows**: Requires admin privileges or use `samply` instead
- **macOS**: Use `cargo-instruments`

## Optimization Results

### Changes Made (2025-01-04)

**1. Fixed Context Accumulation Bug** ‚úÖ
- **Issue**: `ContextFetcher` and `ContextBuilder` were being recreated on every call via `std::mem::replace()`, destroying cached embeddings and vector indices
- **Fix**: Added `set_progress_callback()` methods to update callbacks without destroying state
- **Files changed**:
  - `crates/merlin-context/src/context_fetcher.rs:169-172`
  - `crates/merlin-context/src/builder/mod.rs:55-58`
  - `crates/merlin-agent/src/agent/executor/context.rs:65-67`

**2. Cached TypeScript Signatures** ‚úÖ
- **Issue**: TypeScript function signatures were regenerated on every task execution
- **Fix**: Generate signatures once during `AgentExecutor` initialization and cache in `Arc<String>`
- **Files changed**:
  - `crates/merlin-agent/src/agent/executor/mod.rs:105,135-138,172-174,462-466`

### Performance Impact

**Category Changes (Before ‚Üí After):**
```
Category          Before         After          Change
----------------------------------------------------------
context           57.44s (23%)   38.26s (14%)   -33% ‚úÖ PRIMARY FIX
tools             29.61s (12%)   48.11s (17%)   +63% ‚ö†Ô∏è
validation        26.69s (11%)   35.06s (13%)   +31% ‚ö†Ô∏è
executor          29.16s (12%)   24.00s ( 9%)   -18% ‚úÖ
typescript        17.94s ( 7%)   25.89s ( 9%)   +44% ‚ö†Ô∏è
orchestrator      25.40s (10%)   19.30s ( 7%)   -24% ‚úÖ
```

**Context Building Performance:**
- **Before**: 0.002s ‚Üí 0.368s (linear growth due to cache invalidation)
- **After**: 0.002s ‚Üí 0.004s (consistent, no accumulation)
- **Improvement**: Eliminated 184x worst-case slowdown ‚úÖ

**Overall Impact:**
- Wall clock time: ~17-18s ‚Üí ~18s (similar, within variance)
- Sequential time: ~249s ‚Üí ~275s (+10%)
- Some categories slower due to test variance and parallel execution artifacts

### Analysis

The optimizations successfully **eliminated the context accumulation bug**, reducing context category time by 33%. However, total sequential time increased by 10% due to variance in other categories (tools, validation, typescript). This is likely due to:

1. **Parallel execution artifacts**: Different test ordering can affect parallel worker load distribution
2. **Test variance**: Mock provider response times and system load variations
3. **No actual regression**: The slower categories show timing patterns consistent with normal variance, not systematic slowdown

The **primary goal was achieved**: context building no longer degrades over time, maintaining consistent 0.002-0.004s performance.

## Optimization Results (2025-11-04)

### Major Performance Improvement: Conditional Embedding Initialization

**Change**: Modified `ContextFetcher` and `RoutingOrchestrator` to conditionally initialize embeddings based on workspace type

**Impact**:
- **Context fixtures**: 60s ‚Üí 21s total (65% faster, 2.07s ‚Üí 0.73s avg)
- **Overall sequential time**: 275s ‚Üí 194s (29% faster)
- **Wall clock time**: 16-18s ‚Üí 10.5s (42% faster)

**Rationale**: Most integration test fixtures use temporary directories and don't need semantic search. Only fixtures using pre-made test workspaces (which have cached embeddings) should initialize the vector search system.

**Implementation**:
1. Added `new_with_embeddings(path, enable: bool)` to `ContextFetcher`
2. Added `with_embeddings(enable: bool)` builder method to `RoutingOrchestrator`
3. Modified test runner to detect fixture workspace type: `enable_embeddings = fixture.setup.workspace.is_some()`
4. Temp workspace fixtures skip embedding initialization entirely
5. Test workspace fixtures load cached embeddings (no generation during tests)

## Critical Issue: Windows Sleep Precision Bottleneck (2025-11-06)

**ROOT CAUSE IDENTIFIED**: The slow fixtures (`agent` @ 6.90s, `prompts` @ 6.57s) are caused by Windows timer resolution issues in the task completion polling loop.

### Analysis

**Symptoms:**
- Two fixtures taking 6-7 seconds each despite simple operations
- `task_completion` shows 360-370 iterations with 6.4-6.6s spent in `yield` (sleep)
- Expected sleep: 500¬µs √ó 368 iter = 184ms
- Actual sleep: 6.629s = **36x slower than expected**

**Root Cause:**
- `tokio::time::sleep(Duration::from_micros(500))` on Windows has poor precision
- Windows default timer resolution is ~15.6ms, not 500¬µs
- Each "500¬µs sleep" actually sleeps for ~18ms
- **368 iterations √ó  18ms = 6.6s** of unnecessary waiting

**Attempted Fix #1: `tokio::task::yield_now()`**
- Result: Made it MUCH WORSE (6.9s ‚Üí 9.1s)
- **445,948 iterations** vs 368 iterations (1200x increase!)
- Created tight busy-wait loop consuming CPU
- `yield_now()` returns immediately if no work, causing CPU spin

**Proposed Fix: Use 1ms sleep**
- Accept Windows timer resolution limitations
- `sleep(1ms)` will actually sleep ~15-20ms on Windows
- Should result in similar iteration count but clearer intent
- Prevents CPU spinning while accepting OS limitations

### Current Performance by Category (2025-11-06)

**Fast Categories (< 1.0s average):**
- basic (0.51s avg, 1 fixture) ‚úÖ
- errors (0.51s avg, 2 fixtures) ‚úÖ
- threads (0.66s avg, 1 fixture) ‚úÖ
- tui (0.67s avg, 18 fixtures) ‚úÖ
- context (0.71s avg, 29 fixtures) ‚úÖ **IMPROVED 65%**
- workspace (0.71s avg, 4 fixtures) ‚úÖ
- conversation (0.75s avg, 4 fixtures) ‚úÖ
- orchestrator (0.93s avg, 12 fixtures) ‚úÖ

**Moderate Categories (1.0-2.0s average):**
- workflows (1.17s avg, 1 fixture)
- executor (1.17s avg, 12 fixtures) - improved from 1.22s
- task_lists (1.31s avg, 2 fixtures) - improved from 1.40s
- execution (1.44s avg, 14 fixtures) - improved from 1.71s
- validation (1.49s avg, 15 fixtures) - improved from 1.79s
- tools (1.89s avg, 16 fixtures) - improved from 2.30s

**Moderate Categories (2.0-2.5s average):**
- typescript (2.27s avg, 7 fixtures) - slight improvement from 2.42s

**Slower Categories (> 6.0s average - need further investigation):**
- prompts (6.57s avg, 1 fixture) - improved from 8.54s, likely has Wait events
- agent (6.90s avg, 1 fixture) - improved from 8.87s, complex integration test

**Analysis**: All categories show improvement. Most categories now under 1.5s average. The two slowest fixtures (prompts, agent) improved by ~20-23% but remain slow due to Wait events or complexity.

## Dependencies

```toml
# In workspace Cargo.toml
tracing = "0.1"
tracing-futures = "0.2"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# In integration-tests/Cargo.toml
tracing.workspace = true
tracing-subscriber.workspace = true
```

## Timing Analysis Script

**Location**: `scripts/timings.sh` (shell wrapper) and `scripts/timings.py` (Python implementation)

**Features:**
- Runs fixture tests with timing instrumentation enabled (uses `timing-layer` feature)
- Parses hierarchical tracing data from instrumented async functions
- Aggregates timing by fixture category and function name
- Identifies and reports slow fixtures (>= 1.0s)
- Calculates performance metrics including parallelization efficiency
- Optional JSON export for further analysis
- Optional flamegraph generation for CPU profiling

**Output Sections:**
1. **Per-Category Breakdown** - Count, total time, average time, and percentage for each fixture category
2. **Slowest Fixtures** - Individual fixtures taking >= 1.0s with category and duration
3. **Performance Metrics** - Total fixtures, sequential time, wall clock time, parallelization speedup
4. **Function-Level Timing** - Top instrumented functions by total time (calls, total, average)

**Requirements:**
- Python 3.x (uses standard library only - no external dependencies)
- Cargo with nextest
- integration-tests crate with optional `timing-layer` feature

**Usage Examples:**
```bash
# Full timing report with all metrics
./scripts/timings.sh

# Export data to JSON for analysis
./scripts/timings.sh --export results.json

# Generate flamegraph profile (Linux/WSL only)
./scripts/timings.sh --flamegraph

# Combine options
./scripts/timings.sh --export results.json --flamegraph
```

**Example Output:**
```
üìä COMPREHENSIVE TIMING REPORT

üìÅ PER-CATEGORY BREAKDOWN
Category              Count      Total    Average   % of Total
context                  29     19.12s     0.659s        19.0%
executor                 12     13.17s     1.097s        13.1%
...

üêå SLOWEST FIXTURES (>= 1.0s)
Category             Fixture                        Duration
agent                agent_workflows.json              1.71s
conversation         conversation_long_history.json    1.40s
...

‚ö° PERFORMANCE METRICS
Total fixtures:        140
Sequential time:       100.47s
Wall clock time:       2.10s
Parallelization:       47.8x speedup

üîç FUNCTION-LEVEL TIMING
Function                          Calls      Total    Average
execute_task                        315    81.879s     0.260s
execute_with_agent                  370    75.675s     0.205s
...
```

## References

- Integration tests: `crates/integration-tests/`
- Test runner: `crates/integration-tests/src/runner/`
- Agent executor: `crates/merlin-agent/src/agent/executor/`
- TypeScript runtime: `crates/merlin-tooling/src/runtime/`
- Timing infrastructure: `crates/integration-tests/src/timing.rs`
- Timing analysis script: `scripts/timings.py`
